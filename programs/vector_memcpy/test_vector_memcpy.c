// This file was autogenerated by vector_gen.py
// Do not edit!

#include <stdint.h>
#include <riscv_vector.h>

#ifdef __cplusplus
extern "C" {
#endif
void* memset(void* dest, int ch, size_t count) {
    unsigned char ch_uc = (unsigned char)ch;
    unsigned char* dest_uc = (unsigned char*)dest;
    for (int i = 0; i < count; i++) {
        *(dest_uc + i) = ch_uc;
    }

    return dest_uc;
}
#ifdef __cplusplus
}
#endif

#define ASM_PREG(val) "r"(val)
// GCC doesn't like __has_feature(capabilities), so define a convenience value
// which is only 1 when in LLVM with __has_feature(capabilities)
#define HAS_CAPABILITIES 0

// Patch over differences between GCC, clang, and CHERI-clang
#if defined(__llvm__)
// Clang intrinsics are correct for segmented loads and supports fractional LMUL.
// Clang 14+ has the correct intrinsics for bytemask loads,
// and Clang has been tested with wholereg ASM

    // Use intrinsics for BYTEMASK in newer Clangs, otherwise the intrinsics don't exist
    #if __clang_major__ >= 14
        #define ENABLE_BYTEMASK 1
        #define USE_ASM_FOR_BYTEMASK 0
    #else
        #define ENABLE_BYTEMASK 0
    #endif

    #if __has_feature(capabilities)
        #undef HAS_CAPABILITIES
        #define HAS_CAPABILITIES 1

        #if __has_feature(pure_capabilities)
            // Replace the ASM pointer register function to use capability register
            #undef ASM_PREG
            #define ASM_PREG(val) "C"(val)
        #endif

        // Enable everything
        #define ENABLE_UNIT 1
        #define ENABLE_STRIDED 1
        #define ENABLE_INDEXED 1
        #define ENABLE_MASKED 1
        #define ENABLE_SEGMENTED 1
        #define ENABLE_FRAC_LMUL 1
        #define ENABLE_ASM_WHOLEREG 1
        #define ENABLE_FAULTONLYFIRST 1

        // Use ASM for everything
        #define USE_ASM_FOR_UNIT 1
        #define USE_ASM_FOR_STRIDED 1
        #define USE_ASM_FOR_INDEXED 1
        #define USE_ASM_FOR_MASKED 1
        #define USE_ASM_FOR_SEGMENTED 1
        // WHOLEREG is always ASM - there are no whole reg intrinsics
        #define USE_ASM_FOR_FAULTONLYFIRST 1
    #else
        // Enable everything
        #define ENABLE_UNIT 1
        #define ENABLE_STRIDED 1
        #define ENABLE_INDEXED 1
        #define ENABLE_MASKED 1
        #define ENABLE_SEGMENTED 1
        #define ENABLE_FRAC_LMUL 1
        #define ENABLE_ASM_WHOLEREG 1
        #define ENABLE_FAULTONLYFIRST 1

        // Use intrinsics for everything
        #define USE_ASM_FOR_UNIT 0
        #define USE_ASM_FOR_STRIDED 0
        #define USE_ASM_FOR_INDEXED 0
        #define USE_ASM_FOR_MASKED 0
        #define USE_ASM_FOR_SEGMENTED 0
        // Wholereg has no intrinsics, always ASM
        #define USE_ASM_FOR_FAULTONLYFIRST 0
    #endif
#elif defined(__GNUC__) && !defined(__INTEL_COMPILER)
// GNU exts enabled, not in LLVM or Intel, => in GCC

// My version of GCC intrinsics doesn't have correct intrinsics for segmented loads,
// doesn't support fractional LMUL,
// doesn't have byte-mask intrinsics.

    // Enable everything except fractional LMUL and bytemask
    #define ENABLE_UNIT 1
    #define ENABLE_STRIDED 1
    #define ENABLE_INDEXED 1
    #define ENABLE_MASKED 1
    #define ENABLE_SEGMENTED 1
    #define ENABLE_FRAC_LMUL 0
    #define ENABLE_BYTEMASK 0
    #define ENABLE_ASM_WHOLEREG 1
    #define ENABLE_FAULTONLYFIRST 1

    // Use intrinsics for all except segmented loads
    #define USE_ASM_FOR_UNIT 0
    #define USE_ASM_FOR_STRIDED 0
    #define USE_ASM_FOR_INDEXED 0
    #define USE_ASM_FOR_MASKED 0
    #define USE_ASM_FOR_SEGMENTED 1
    #define USE_ASM_FOR_BYTEMASK 1
    // Wholereg is always ASM
    #define USE_ASM_FOR_FAULTONLYFIRST 1
#endif

volatile extern int64_t outputAttempted; // magic output device
volatile extern int64_t outputSucceeded; // magic output device
volatile extern int64_t ramBoundary; // edge of writable memory
int64_t vector_memcpy_harness_uint8_t(void (*memcpy_fn)(size_t, const uint8_t* __restrict__, uint8_t* __restrict__)) {
    uint8_t data[128] = {0};
    uint8_t out_data[128] = {0};
    
    for (uint8_t i = 0; i < 128; i++) {
        data[i] = i;
    }
    
    // ONLY copy 110 elements
    memcpy_fn(110, data, out_data);
    
    // Check the first 110 elements of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint8_t i = 0; i < 110; i++) {
        if (data[i] != out_data[i]) {
            return 0;
        }
    }
    // Check that the rest are 0 (the original value)
    // This ensures that the emulator didn't store more elements than it should have
    for (uint8_t i = 110; i < 128; i++) {
        if (out_data[i] != 0) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_harness_uint16_t(void (*memcpy_fn)(size_t, const uint16_t* __restrict__, uint16_t* __restrict__)) {
    uint16_t data[128] = {0};
    uint16_t out_data[128] = {0};
    
    for (uint16_t i = 0; i < 128; i++) {
        data[i] = i;
    }
    
    // ONLY copy 110 elements
    memcpy_fn(110, data, out_data);
    
    // Check the first 110 elements of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint16_t i = 0; i < 110; i++) {
        if (data[i] != out_data[i]) {
            return 0;
        }
    }
    // Check that the rest are 0 (the original value)
    // This ensures that the emulator didn't store more elements than it should have
    for (uint16_t i = 110; i < 128; i++) {
        if (out_data[i] != 0) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_harness_uint32_t(void (*memcpy_fn)(size_t, const uint32_t* __restrict__, uint32_t* __restrict__)) {
    uint32_t data[128] = {0};
    uint32_t out_data[128] = {0};
    
    for (uint32_t i = 0; i < 128; i++) {
        data[i] = i;
    }
    
    // ONLY copy 110 elements
    memcpy_fn(110, data, out_data);
    
    // Check the first 110 elements of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint32_t i = 0; i < 110; i++) {
        if (data[i] != out_data[i]) {
            return 0;
        }
    }
    // Check that the rest are 0 (the original value)
    // This ensures that the emulator didn't store more elements than it should have
    for (uint32_t i = 110; i < 128; i++) {
        if (out_data[i] != 0) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_harness_uint64_t(void (*memcpy_fn)(size_t, const uint64_t* __restrict__, uint64_t* __restrict__)) {
    uint64_t data[128] = {0};
    uint64_t out_data[128] = {0};
    
    for (uint64_t i = 0; i < 128; i++) {
        data[i] = i;
    }
    
    // ONLY copy 110 elements
    memcpy_fn(110, data, out_data);
    
    // Check the first 110 elements of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint64_t i = 0; i < 110; i++) {
        if (data[i] != out_data[i]) {
            return 0;
        }
    }
    // Check that the rest are 0 (the original value)
    // This ensures that the emulator didn't store more elements than it should have
    for (uint64_t i = 110; i < 128; i++) {
        if (out_data[i] != 0) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_masked_harness_uint8_t(void (*memcpy_fn)(size_t, const uint8_t* __restrict__, uint8_t* __restrict__)) {
    uint8_t data[128] = {0};
    uint8_t out_data[128] = {0};
    const uint8_t SENTINEL_NOT_WRITTEN = 0xbb;
    
    for (uint8_t i = 0; i < 128; i++) {
        data[i] = i;
        out_data[i] = SENTINEL_NOT_WRITTEN;
    }
    
    // ONLY copy 110 elements
    // For the masked function, this should only copy odd-indexed elements.
    memcpy_fn(110, data, out_data);
    
    // Check the first 110 elements of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint8_t i = 0; i < 110; i++) {
        if ((i & 1) == 1 && data[i] != out_data[i]) {
            return 0;
        } else if ((i & 1) == 0 && out_data[i] != SENTINEL_NOT_WRITTEN) {
            return 0;
        }
    }
    // Check that the rest are all the original value
    // This ensures that the emulator didn't store more elements than it should have
    for (uint8_t i = 110; i < 128; i++) {
        if (out_data[i] != SENTINEL_NOT_WRITTEN) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_masked_harness_uint16_t(void (*memcpy_fn)(size_t, const uint16_t* __restrict__, uint16_t* __restrict__)) {
    uint16_t data[128] = {0};
    uint16_t out_data[128] = {0};
    const uint16_t SENTINEL_NOT_WRITTEN = 0xbb;
    
    for (uint16_t i = 0; i < 128; i++) {
        data[i] = i;
        out_data[i] = SENTINEL_NOT_WRITTEN;
    }
    
    // ONLY copy 110 elements
    // For the masked function, this should only copy odd-indexed elements.
    memcpy_fn(110, data, out_data);
    
    // Check the first 110 elements of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint16_t i = 0; i < 110; i++) {
        if ((i & 1) == 1 && data[i] != out_data[i]) {
            return 0;
        } else if ((i & 1) == 0 && out_data[i] != SENTINEL_NOT_WRITTEN) {
            return 0;
        }
    }
    // Check that the rest are all the original value
    // This ensures that the emulator didn't store more elements than it should have
    for (uint16_t i = 110; i < 128; i++) {
        if (out_data[i] != SENTINEL_NOT_WRITTEN) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_masked_harness_uint32_t(void (*memcpy_fn)(size_t, const uint32_t* __restrict__, uint32_t* __restrict__)) {
    uint32_t data[128] = {0};
    uint32_t out_data[128] = {0};
    const uint32_t SENTINEL_NOT_WRITTEN = 0xbb;
    
    for (uint32_t i = 0; i < 128; i++) {
        data[i] = i;
        out_data[i] = SENTINEL_NOT_WRITTEN;
    }
    
    // ONLY copy 110 elements
    // For the masked function, this should only copy odd-indexed elements.
    memcpy_fn(110, data, out_data);
    
    // Check the first 110 elements of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint32_t i = 0; i < 110; i++) {
        if ((i & 1) == 1 && data[i] != out_data[i]) {
            return 0;
        } else if ((i & 1) == 0 && out_data[i] != SENTINEL_NOT_WRITTEN) {
            return 0;
        }
    }
    // Check that the rest are all the original value
    // This ensures that the emulator didn't store more elements than it should have
    for (uint32_t i = 110; i < 128; i++) {
        if (out_data[i] != SENTINEL_NOT_WRITTEN) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_masked_harness_uint64_t(void (*memcpy_fn)(size_t, const uint64_t* __restrict__, uint64_t* __restrict__)) {
    uint64_t data[128] = {0};
    uint64_t out_data[128] = {0};
    const uint64_t SENTINEL_NOT_WRITTEN = 0xbb;
    
    for (uint64_t i = 0; i < 128; i++) {
        data[i] = i;
        out_data[i] = SENTINEL_NOT_WRITTEN;
    }
    
    // ONLY copy 110 elements
    // For the masked function, this should only copy odd-indexed elements.
    memcpy_fn(110, data, out_data);
    
    // Check the first 110 elements of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint64_t i = 0; i < 110; i++) {
        if ((i & 1) == 1 && data[i] != out_data[i]) {
            return 0;
        } else if ((i & 1) == 0 && out_data[i] != SENTINEL_NOT_WRITTEN) {
            return 0;
        }
    }
    // Check that the rest are all the original value
    // This ensures that the emulator didn't store more elements than it should have
    for (uint64_t i = 110; i < 128; i++) {
        if (out_data[i] != SENTINEL_NOT_WRITTEN) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_segmented_harness_uint8_t(void (*memcpy_fn)(size_t, const uint8_t* __restrict__, uint8_t* __restrict__[4])) {
    uint8_t data[128] = {0};
    uint8_t out_r[32] = {0};
    uint8_t out_g[32] = {0};
    uint8_t out_b[32] = {0};
    uint8_t out_a[32] = {0};
    
    for (uint8_t i = 0; i < 128; i++) {
        data[i] = i;
    }
    
    uint8_t* out_datas[4] = {out_r, out_g, out_b, out_a};
    
    
    // ONLY copy 104 elements = 26 segments
    // For the masked function, this should only copy odd-indexed elements.
    memcpy_fn(26, data, out_datas);
    
    // Check the first 104 elements = 26 segments of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint8_t i = 0; i < 26; i++) {
        if (data[i*4 + 0] != out_r[i]) {
            return 0;
        }
        if (data[i*4 + 1] != out_g[i]) {
            return 0;
        }
        if (data[i*4 + 2] != out_b[i]) {
            return 0;
        }
        if (data[i*4 + 3] != out_a[i]) {
            return 0;
        }
    }
    // Check that the rest are 0 (the original value)
    // This ensures that the emulator didn't store more elements than it should have
    for (uint8_t i = 26; i < 32; i++) {
        if (out_r[i] != 0 || out_g[i] != 0 || out_b[i] != 0 || out_a[i] != 0) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_segmented_harness_uint16_t(void (*memcpy_fn)(size_t, const uint16_t* __restrict__, uint16_t* __restrict__[4])) {
    uint16_t data[128] = {0};
    uint16_t out_r[32] = {0};
    uint16_t out_g[32] = {0};
    uint16_t out_b[32] = {0};
    uint16_t out_a[32] = {0};
    
    for (uint16_t i = 0; i < 128; i++) {
        data[i] = i;
    }
    
    uint16_t* out_datas[4] = {out_r, out_g, out_b, out_a};
    
    
    // ONLY copy 104 elements = 26 segments
    // For the masked function, this should only copy odd-indexed elements.
    memcpy_fn(26, data, out_datas);
    
    // Check the first 104 elements = 26 segments of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint16_t i = 0; i < 26; i++) {
        if (data[i*4 + 0] != out_r[i]) {
            return 0;
        }
        if (data[i*4 + 1] != out_g[i]) {
            return 0;
        }
        if (data[i*4 + 2] != out_b[i]) {
            return 0;
        }
        if (data[i*4 + 3] != out_a[i]) {
            return 0;
        }
    }
    // Check that the rest are 0 (the original value)
    // This ensures that the emulator didn't store more elements than it should have
    for (uint16_t i = 26; i < 32; i++) {
        if (out_r[i] != 0 || out_g[i] != 0 || out_b[i] != 0 || out_a[i] != 0) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_segmented_harness_uint32_t(void (*memcpy_fn)(size_t, const uint32_t* __restrict__, uint32_t* __restrict__[4])) {
    uint32_t data[128] = {0};
    uint32_t out_r[32] = {0};
    uint32_t out_g[32] = {0};
    uint32_t out_b[32] = {0};
    uint32_t out_a[32] = {0};
    
    for (uint32_t i = 0; i < 128; i++) {
        data[i] = i;
    }
    
    uint32_t* out_datas[4] = {out_r, out_g, out_b, out_a};
    
    
    // ONLY copy 104 elements = 26 segments
    // For the masked function, this should only copy odd-indexed elements.
    memcpy_fn(26, data, out_datas);
    
    // Check the first 104 elements = 26 segments of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint32_t i = 0; i < 26; i++) {
        if (data[i*4 + 0] != out_r[i]) {
            return 0;
        }
        if (data[i*4 + 1] != out_g[i]) {
            return 0;
        }
        if (data[i*4 + 2] != out_b[i]) {
            return 0;
        }
        if (data[i*4 + 3] != out_a[i]) {
            return 0;
        }
    }
    // Check that the rest are 0 (the original value)
    // This ensures that the emulator didn't store more elements than it should have
    for (uint32_t i = 26; i < 32; i++) {
        if (out_r[i] != 0 || out_g[i] != 0 || out_b[i] != 0 || out_a[i] != 0) {
            return 0;
        }
    }
    return 1;
}
int64_t vector_memcpy_segmented_harness_uint64_t(void (*memcpy_fn)(size_t, const uint64_t* __restrict__, uint64_t* __restrict__[4])) {
    uint64_t data[128] = {0};
    uint64_t out_r[32] = {0};
    uint64_t out_g[32] = {0};
    uint64_t out_b[32] = {0};
    uint64_t out_a[32] = {0};
    
    for (uint64_t i = 0; i < 128; i++) {
        data[i] = i;
    }
    
    uint64_t* out_datas[4] = {out_r, out_g, out_b, out_a};
    
    
    // ONLY copy 104 elements = 26 segments
    // For the masked function, this should only copy odd-indexed elements.
    memcpy_fn(26, data, out_datas);
    
    // Check the first 104 elements = 26 segments of output are the same
    // This ensures that the emulator correctly loaded/stored enough values
    for (uint64_t i = 0; i < 26; i++) {
        if (data[i*4 + 0] != out_r[i]) {
            return 0;
        }
        if (data[i*4 + 1] != out_g[i]) {
            return 0;
        }
        if (data[i*4 + 2] != out_b[i]) {
            return 0;
        }
        if (data[i*4 + 3] != out_a[i]) {
            return 0;
        }
    }
    // Check that the rest are 0 (the original value)
    // This ensures that the emulator didn't store more elements than it should have
    for (uint64_t i = 26; i < 32; i++) {
        if (out_r[i] != 0 || out_g[i] != 0 || out_b[i] != 0 || out_a[i] != 0) {
            return 0;
        }
    }
    return 1;
}
#if ENABLE_UNIT
void vector_memcpy_unit_stride_e8m1(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8m1(n);
            if (copied_per_iter == 0) break;
            vuint8m1_t data;
            #if USE_ASM_FOR_UNIT
            asm volatile ("vle8.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse8.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle8_v_u8m1(in, copied_per_iter);
            vse8_v_u8m1(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_UNIT
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_UNIT
#if ENABLE_UNIT
void vector_memcpy_unit_stride_e16m2(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16m2(n);
            if (copied_per_iter == 0) break;
            vuint16m2_t data;
            #if USE_ASM_FOR_UNIT
            asm volatile ("vle16.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse16.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle16_v_u16m2(in, copied_per_iter);
            vse16_v_u16m2(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_UNIT
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_UNIT
#if ENABLE_UNIT
void vector_memcpy_unit_stride_e32m4(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32m4(n);
            if (copied_per_iter == 0) break;
            vuint32m4_t data;
            #if USE_ASM_FOR_UNIT
            asm volatile ("vle32.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse32.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle32_v_u32m4(in, copied_per_iter);
            vse32_v_u32m4(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_UNIT
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_UNIT
#if ENABLE_UNIT
void vector_memcpy_unit_stride_e64m8(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m8(n);
            if (copied_per_iter == 0) break;
            vuint64m8_t data;
            #if USE_ASM_FOR_UNIT
            asm volatile ("vle64.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse64.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle64_v_u64m8(in, copied_per_iter);
            vse64_v_u64m8(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_UNIT
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_UNIT
#if ENABLE_UNIT && ENABLE_FRAC_LMUL
void vector_memcpy_unit_stride_e32mf2(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32mf2(n);
            if (copied_per_iter == 0) break;
            vuint32mf2_t data;
            #if USE_ASM_FOR_UNIT
            asm volatile ("vle32.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse32.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle32_v_u32mf2(in, copied_per_iter);
            vse32_v_u32mf2(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_UNIT
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_UNIT && ENABLE_FRAC_LMUL
#if ENABLE_UNIT && ENABLE_FRAC_LMUL
void vector_memcpy_unit_stride_e16mf4(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16mf4(n);
            if (copied_per_iter == 0) break;
            vuint16mf4_t data;
            #if USE_ASM_FOR_UNIT
            asm volatile ("vle16.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse16.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle16_v_u16mf4(in, copied_per_iter);
            vse16_v_u16mf4(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_UNIT
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_UNIT && ENABLE_FRAC_LMUL
#if ENABLE_UNIT && ENABLE_FRAC_LMUL
void vector_memcpy_unit_stride_e8mf8(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8mf8(n);
            if (copied_per_iter == 0) break;
            vuint8mf8_t data;
            #if USE_ASM_FOR_UNIT
            asm volatile ("vle8.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse8.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle8_v_u8mf8(in, copied_per_iter);
            vse8_v_u8mf8(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_UNIT
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_UNIT && ENABLE_FRAC_LMUL
#if ENABLE_STRIDED
void vector_memcpy_strided_e8m1(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    const size_t STRIDE_ELEMS = 4;
    const size_t STRIDE_BYTES = 4 * sizeof(uint8_t);
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8m1(n);
            if (copied_per_iter == 0) break;
            vuint8m1_t data;
            if (copied_per_iter * STRIDE_ELEMS < n) {
                for (size_t i = 0; i < STRIDE_ELEMS; i++) {
                    const uint8_t* in_offset = in + i;
                    uint8_t* out_offset = out + i;
                    #if USE_ASM_FOR_STRIDED
                    asm volatile ("vlse8.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in_offset), "r"(STRIDE_BYTES));
                    asm volatile ("vsse8.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out_offset), "r"(STRIDE_BYTES));
                    #else
                    data = vlse8_v_u8m1(in_offset, STRIDE_BYTES, copied_per_iter);
                    vsse8_v_u8m1(out_offset, STRIDE_BYTES, data, copied_per_iter);
                    #endif // USE_ASM_FOR_STRIDED
                }
                in += copied_per_iter * STRIDE_ELEMS;
                out += copied_per_iter * STRIDE_ELEMS;
                n -= copied_per_iter * STRIDE_ELEMS;
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle8.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse8.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle8_v_u8m1(in, copied_per_iter);
                vse8_v_u8m1(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
                in += copied_per_iter;
                out += copied_per_iter;
                n -= copied_per_iter;
            }
        }
    }
}
#endif // ENABLE_STRIDED
#if ENABLE_STRIDED
void vector_memcpy_strided_e16m2(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    const size_t STRIDE_ELEMS = 4;
    const size_t STRIDE_BYTES = 4 * sizeof(uint16_t);
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16m2(n);
            if (copied_per_iter == 0) break;
            vuint16m2_t data;
            if (copied_per_iter * STRIDE_ELEMS < n) {
                for (size_t i = 0; i < STRIDE_ELEMS; i++) {
                    const uint16_t* in_offset = in + i;
                    uint16_t* out_offset = out + i;
                    #if USE_ASM_FOR_STRIDED
                    asm volatile ("vlse16.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in_offset), "r"(STRIDE_BYTES));
                    asm volatile ("vsse16.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out_offset), "r"(STRIDE_BYTES));
                    #else
                    data = vlse16_v_u16m2(in_offset, STRIDE_BYTES, copied_per_iter);
                    vsse16_v_u16m2(out_offset, STRIDE_BYTES, data, copied_per_iter);
                    #endif // USE_ASM_FOR_STRIDED
                }
                in += copied_per_iter * STRIDE_ELEMS;
                out += copied_per_iter * STRIDE_ELEMS;
                n -= copied_per_iter * STRIDE_ELEMS;
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle16.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse16.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle16_v_u16m2(in, copied_per_iter);
                vse16_v_u16m2(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
                in += copied_per_iter;
                out += copied_per_iter;
                n -= copied_per_iter;
            }
        }
    }
}
#endif // ENABLE_STRIDED
#if ENABLE_STRIDED
void vector_memcpy_strided_e32m4(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    const size_t STRIDE_ELEMS = 4;
    const size_t STRIDE_BYTES = 4 * sizeof(uint32_t);
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32m4(n);
            if (copied_per_iter == 0) break;
            vuint32m4_t data;
            if (copied_per_iter * STRIDE_ELEMS < n) {
                for (size_t i = 0; i < STRIDE_ELEMS; i++) {
                    const uint32_t* in_offset = in + i;
                    uint32_t* out_offset = out + i;
                    #if USE_ASM_FOR_STRIDED
                    asm volatile ("vlse32.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in_offset), "r"(STRIDE_BYTES));
                    asm volatile ("vsse32.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out_offset), "r"(STRIDE_BYTES));
                    #else
                    data = vlse32_v_u32m4(in_offset, STRIDE_BYTES, copied_per_iter);
                    vsse32_v_u32m4(out_offset, STRIDE_BYTES, data, copied_per_iter);
                    #endif // USE_ASM_FOR_STRIDED
                }
                in += copied_per_iter * STRIDE_ELEMS;
                out += copied_per_iter * STRIDE_ELEMS;
                n -= copied_per_iter * STRIDE_ELEMS;
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle32.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse32.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle32_v_u32m4(in, copied_per_iter);
                vse32_v_u32m4(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
                in += copied_per_iter;
                out += copied_per_iter;
                n -= copied_per_iter;
            }
        }
    }
}
#endif // ENABLE_STRIDED
#if ENABLE_STRIDED
void vector_memcpy_strided_e64m8(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    const size_t STRIDE_ELEMS = 4;
    const size_t STRIDE_BYTES = 4 * sizeof(uint64_t);
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m8(n);
            if (copied_per_iter == 0) break;
            vuint64m8_t data;
            if (copied_per_iter * STRIDE_ELEMS < n) {
                for (size_t i = 0; i < STRIDE_ELEMS; i++) {
                    const uint64_t* in_offset = in + i;
                    uint64_t* out_offset = out + i;
                    #if USE_ASM_FOR_STRIDED
                    asm volatile ("vlse64.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in_offset), "r"(STRIDE_BYTES));
                    asm volatile ("vsse64.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out_offset), "r"(STRIDE_BYTES));
                    #else
                    data = vlse64_v_u64m8(in_offset, STRIDE_BYTES, copied_per_iter);
                    vsse64_v_u64m8(out_offset, STRIDE_BYTES, data, copied_per_iter);
                    #endif // USE_ASM_FOR_STRIDED
                }
                in += copied_per_iter * STRIDE_ELEMS;
                out += copied_per_iter * STRIDE_ELEMS;
                n -= copied_per_iter * STRIDE_ELEMS;
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle64.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse64.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle64_v_u64m8(in, copied_per_iter);
                vse64_v_u64m8(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
                in += copied_per_iter;
                out += copied_per_iter;
                n -= copied_per_iter;
            }
        }
    }
}
#endif // ENABLE_STRIDED
#if ENABLE_STRIDED && ENABLE_FRAC_LMUL
void vector_memcpy_strided_e32mf2(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    const size_t STRIDE_ELEMS = 4;
    const size_t STRIDE_BYTES = 4 * sizeof(uint32_t);
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32mf2(n);
            if (copied_per_iter == 0) break;
            vuint32mf2_t data;
            if (copied_per_iter * STRIDE_ELEMS < n) {
                for (size_t i = 0; i < STRIDE_ELEMS; i++) {
                    const uint32_t* in_offset = in + i;
                    uint32_t* out_offset = out + i;
                    #if USE_ASM_FOR_STRIDED
                    asm volatile ("vlse32.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in_offset), "r"(STRIDE_BYTES));
                    asm volatile ("vsse32.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out_offset), "r"(STRIDE_BYTES));
                    #else
                    data = vlse32_v_u32mf2(in_offset, STRIDE_BYTES, copied_per_iter);
                    vsse32_v_u32mf2(out_offset, STRIDE_BYTES, data, copied_per_iter);
                    #endif // USE_ASM_FOR_STRIDED
                }
                in += copied_per_iter * STRIDE_ELEMS;
                out += copied_per_iter * STRIDE_ELEMS;
                n -= copied_per_iter * STRIDE_ELEMS;
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle32.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse32.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle32_v_u32mf2(in, copied_per_iter);
                vse32_v_u32mf2(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
                in += copied_per_iter;
                out += copied_per_iter;
                n -= copied_per_iter;
            }
        }
    }
}
#endif // ENABLE_STRIDED && ENABLE_FRAC_LMUL
#if ENABLE_STRIDED && ENABLE_FRAC_LMUL
void vector_memcpy_strided_e16mf4(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    const size_t STRIDE_ELEMS = 4;
    const size_t STRIDE_BYTES = 4 * sizeof(uint16_t);
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16mf4(n);
            if (copied_per_iter == 0) break;
            vuint16mf4_t data;
            if (copied_per_iter * STRIDE_ELEMS < n) {
                for (size_t i = 0; i < STRIDE_ELEMS; i++) {
                    const uint16_t* in_offset = in + i;
                    uint16_t* out_offset = out + i;
                    #if USE_ASM_FOR_STRIDED
                    asm volatile ("vlse16.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in_offset), "r"(STRIDE_BYTES));
                    asm volatile ("vsse16.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out_offset), "r"(STRIDE_BYTES));
                    #else
                    data = vlse16_v_u16mf4(in_offset, STRIDE_BYTES, copied_per_iter);
                    vsse16_v_u16mf4(out_offset, STRIDE_BYTES, data, copied_per_iter);
                    #endif // USE_ASM_FOR_STRIDED
                }
                in += copied_per_iter * STRIDE_ELEMS;
                out += copied_per_iter * STRIDE_ELEMS;
                n -= copied_per_iter * STRIDE_ELEMS;
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle16.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse16.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle16_v_u16mf4(in, copied_per_iter);
                vse16_v_u16mf4(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
                in += copied_per_iter;
                out += copied_per_iter;
                n -= copied_per_iter;
            }
        }
    }
}
#endif // ENABLE_STRIDED && ENABLE_FRAC_LMUL
#if ENABLE_STRIDED && ENABLE_FRAC_LMUL
void vector_memcpy_strided_e8mf8(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    const size_t STRIDE_ELEMS = 4;
    const size_t STRIDE_BYTES = 4 * sizeof(uint8_t);
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8mf8(n);
            if (copied_per_iter == 0) break;
            vuint8mf8_t data;
            if (copied_per_iter * STRIDE_ELEMS < n) {
                for (size_t i = 0; i < STRIDE_ELEMS; i++) {
                    const uint8_t* in_offset = in + i;
                    uint8_t* out_offset = out + i;
                    #if USE_ASM_FOR_STRIDED
                    asm volatile ("vlse8.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in_offset), "r"(STRIDE_BYTES));
                    asm volatile ("vsse8.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out_offset), "r"(STRIDE_BYTES));
                    #else
                    data = vlse8_v_u8mf8(in_offset, STRIDE_BYTES, copied_per_iter);
                    vsse8_v_u8mf8(out_offset, STRIDE_BYTES, data, copied_per_iter);
                    #endif // USE_ASM_FOR_STRIDED
                }
                in += copied_per_iter * STRIDE_ELEMS;
                out += copied_per_iter * STRIDE_ELEMS;
                n -= copied_per_iter * STRIDE_ELEMS;
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle8.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse8.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle8_v_u8mf8(in, copied_per_iter);
                vse8_v_u8mf8(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
                in += copied_per_iter;
                out += copied_per_iter;
                n -= copied_per_iter;
            }
        }
    }
}
#endif // ENABLE_STRIDED && ENABLE_FRAC_LMUL
#if ENABLE_INDEXED
void vector_memcpy_indexed_e8m1(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    const size_t ELEM_WIDTH = sizeof(uint8_t);
    const size_t VLMAX = vsetvlmax_e8m1();
    uint8_t indices[128] = {0};
    for (size_t i = 0; i < VLMAX; i++) {
        indices[i] = (((uint8_t) i) ^ 1) * ELEM_WIDTH;
    }
    vuint8m1_t indices_v;
    #if HAS_CAPABILITIES
    asm volatile ("vle8.v %0, (%1)" : "=vr"(indices_v) : ASM_PREG(indices));
    #else
    indices_v = vle8_v_u8m1(indices, VLMAX);
    #endif
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8m1(n);
            if (copied_per_iter == 0) break;
            vuint8m1_t data;
            if (copied_per_iter == VLMAX) {
                #if USE_ASM_FOR_INDEXED
                asm volatile ("vluxei8.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in), "vr"(indices_v));
                asm volatile ("vsuxei8.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out), "vr"(indices_v));
                #else
                data = vluxei8_v_u8m1(in, indices_v, copied_per_iter);
                vsuxei8_v_u8m1(out, indices_v, data, copied_per_iter);
                #endif // USE_ASM_FOR_INDEXED
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle8.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse8.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle8_v_u8m1(in, copied_per_iter);
                vse8_v_u8m1(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_INDEXED
#if ENABLE_INDEXED
void vector_memcpy_indexed_e16m2(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    const size_t ELEM_WIDTH = sizeof(uint16_t);
    const size_t VLMAX = vsetvlmax_e16m2();
    uint16_t indices[128] = {0};
    for (size_t i = 0; i < VLMAX; i++) {
        indices[i] = (((uint16_t) i) ^ 1) * ELEM_WIDTH;
    }
    vuint16m2_t indices_v;
    #if HAS_CAPABILITIES
    asm volatile ("vle16.v %0, (%1)" : "=vr"(indices_v) : ASM_PREG(indices));
    #else
    indices_v = vle16_v_u16m2(indices, VLMAX);
    #endif
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16m2(n);
            if (copied_per_iter == 0) break;
            vuint16m2_t data;
            if (copied_per_iter == VLMAX) {
                #if USE_ASM_FOR_INDEXED
                asm volatile ("vluxei16.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in), "vr"(indices_v));
                asm volatile ("vsuxei16.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out), "vr"(indices_v));
                #else
                data = vluxei16_v_u16m2(in, indices_v, copied_per_iter);
                vsuxei16_v_u16m2(out, indices_v, data, copied_per_iter);
                #endif // USE_ASM_FOR_INDEXED
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle16.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse16.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle16_v_u16m2(in, copied_per_iter);
                vse16_v_u16m2(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_INDEXED
#if ENABLE_INDEXED
void vector_memcpy_indexed_e32m4(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    const size_t ELEM_WIDTH = sizeof(uint32_t);
    const size_t VLMAX = vsetvlmax_e32m4();
    uint32_t indices[128] = {0};
    for (size_t i = 0; i < VLMAX; i++) {
        indices[i] = (((uint32_t) i) ^ 1) * ELEM_WIDTH;
    }
    vuint32m4_t indices_v;
    #if HAS_CAPABILITIES
    asm volatile ("vle32.v %0, (%1)" : "=vr"(indices_v) : ASM_PREG(indices));
    #else
    indices_v = vle32_v_u32m4(indices, VLMAX);
    #endif
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32m4(n);
            if (copied_per_iter == 0) break;
            vuint32m4_t data;
            if (copied_per_iter == VLMAX) {
                #if USE_ASM_FOR_INDEXED
                asm volatile ("vluxei32.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in), "vr"(indices_v));
                asm volatile ("vsuxei32.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out), "vr"(indices_v));
                #else
                data = vluxei32_v_u32m4(in, indices_v, copied_per_iter);
                vsuxei32_v_u32m4(out, indices_v, data, copied_per_iter);
                #endif // USE_ASM_FOR_INDEXED
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle32.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse32.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle32_v_u32m4(in, copied_per_iter);
                vse32_v_u32m4(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_INDEXED
#if ENABLE_INDEXED
void vector_memcpy_indexed_e64m8(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    const size_t ELEM_WIDTH = sizeof(uint64_t);
    const size_t VLMAX = vsetvlmax_e64m8();
    uint64_t indices[128] = {0};
    for (size_t i = 0; i < VLMAX; i++) {
        indices[i] = (((uint64_t) i) ^ 1) * ELEM_WIDTH;
    }
    vuint64m8_t indices_v;
    #if HAS_CAPABILITIES
    asm volatile ("vle64.v %0, (%1)" : "=vr"(indices_v) : ASM_PREG(indices));
    #else
    indices_v = vle64_v_u64m8(indices, VLMAX);
    #endif
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m8(n);
            if (copied_per_iter == 0) break;
            vuint64m8_t data;
            if (copied_per_iter == VLMAX) {
                #if USE_ASM_FOR_INDEXED
                asm volatile ("vluxei64.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in), "vr"(indices_v));
                asm volatile ("vsuxei64.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out), "vr"(indices_v));
                #else
                data = vluxei64_v_u64m8(in, indices_v, copied_per_iter);
                vsuxei64_v_u64m8(out, indices_v, data, copied_per_iter);
                #endif // USE_ASM_FOR_INDEXED
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle64.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse64.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle64_v_u64m8(in, copied_per_iter);
                vse64_v_u64m8(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_INDEXED
#if ENABLE_INDEXED && ENABLE_FRAC_LMUL
void vector_memcpy_indexed_e32mf2(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    const size_t ELEM_WIDTH = sizeof(uint32_t);
    const size_t VLMAX = vsetvlmax_e32mf2();
    uint32_t indices[128] = {0};
    for (size_t i = 0; i < VLMAX; i++) {
        indices[i] = (((uint32_t) i) ^ 1) * ELEM_WIDTH;
    }
    vuint32mf2_t indices_v;
    #if HAS_CAPABILITIES
    asm volatile ("vle32.v %0, (%1)" : "=vr"(indices_v) : ASM_PREG(indices));
    #else
    indices_v = vle32_v_u32mf2(indices, VLMAX);
    #endif
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32mf2(n);
            if (copied_per_iter == 0) break;
            vuint32mf2_t data;
            if (copied_per_iter == VLMAX) {
                #if USE_ASM_FOR_INDEXED
                asm volatile ("vluxei32.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in), "vr"(indices_v));
                asm volatile ("vsuxei32.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out), "vr"(indices_v));
                #else
                data = vluxei32_v_u32mf2(in, indices_v, copied_per_iter);
                vsuxei32_v_u32mf2(out, indices_v, data, copied_per_iter);
                #endif // USE_ASM_FOR_INDEXED
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle32.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse32.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle32_v_u32mf2(in, copied_per_iter);
                vse32_v_u32mf2(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_INDEXED && ENABLE_FRAC_LMUL
#if ENABLE_INDEXED && ENABLE_FRAC_LMUL
void vector_memcpy_indexed_e16mf4(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    const size_t ELEM_WIDTH = sizeof(uint16_t);
    const size_t VLMAX = vsetvlmax_e16mf4();
    uint16_t indices[128] = {0};
    for (size_t i = 0; i < VLMAX; i++) {
        indices[i] = (((uint16_t) i) ^ 1) * ELEM_WIDTH;
    }
    vuint16mf4_t indices_v;
    #if HAS_CAPABILITIES
    asm volatile ("vle16.v %0, (%1)" : "=vr"(indices_v) : ASM_PREG(indices));
    #else
    indices_v = vle16_v_u16mf4(indices, VLMAX);
    #endif
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16mf4(n);
            if (copied_per_iter == 0) break;
            vuint16mf4_t data;
            if (copied_per_iter == VLMAX) {
                #if USE_ASM_FOR_INDEXED
                asm volatile ("vluxei16.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in), "vr"(indices_v));
                asm volatile ("vsuxei16.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out), "vr"(indices_v));
                #else
                data = vluxei16_v_u16mf4(in, indices_v, copied_per_iter);
                vsuxei16_v_u16mf4(out, indices_v, data, copied_per_iter);
                #endif // USE_ASM_FOR_INDEXED
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle16.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse16.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle16_v_u16mf4(in, copied_per_iter);
                vse16_v_u16mf4(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_INDEXED && ENABLE_FRAC_LMUL
#if ENABLE_INDEXED && ENABLE_FRAC_LMUL
void vector_memcpy_indexed_e8mf8(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    const size_t ELEM_WIDTH = sizeof(uint8_t);
    const size_t VLMAX = vsetvlmax_e8mf8();
    uint8_t indices[128] = {0};
    for (size_t i = 0; i < VLMAX; i++) {
        indices[i] = (((uint8_t) i) ^ 1) * ELEM_WIDTH;
    }
    vuint8mf8_t indices_v;
    #if HAS_CAPABILITIES
    asm volatile ("vle8.v %0, (%1)" : "=vr"(indices_v) : ASM_PREG(indices));
    #else
    indices_v = vle8_v_u8mf8(indices, VLMAX);
    #endif
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8mf8(n);
            if (copied_per_iter == 0) break;
            vuint8mf8_t data;
            if (copied_per_iter == VLMAX) {
                #if USE_ASM_FOR_INDEXED
                asm volatile ("vluxei8.v %0, (%1), %2" : "=vr"(data) : ASM_PREG(in), "vr"(indices_v));
                asm volatile ("vsuxei8.v %0, (%1), %2" :: "vr"(data),  ASM_PREG(out), "vr"(indices_v));
                #else
                data = vluxei8_v_u8mf8(in, indices_v, copied_per_iter);
                vsuxei8_v_u8mf8(out, indices_v, data, copied_per_iter);
                #endif // USE_ASM_FOR_INDEXED
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle8.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse8.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle8_v_u8mf8(in, copied_per_iter);
                vse8_v_u8mf8(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_INDEXED && ENABLE_FRAC_LMUL
#if ENABLE_MASKED
void vector_memcpy_masked_e8m1(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    uint8_t mask_ints[128] = {0};
    const size_t VLMAX = vsetvlmax_e8m1();
    for (size_t i = 0; i < VLMAX; i++) {
        mask_ints[i] = i & 1;
    }
    vuint8m1_t mask_ints_v;
    #if USE_ASM_FOR_UNIT
    asm volatile ("vle8.v %0, (%1)" : "=vr"(mask_ints_v) : ASM_PREG(&mask_ints[0]));
    #else
    mask_ints_v = vle8_v_u8m1(mask_ints, VLMAX);
    #endif // USE_ASM_FOR_UNIT
    vbool8_t mask = vmseq_vx_u8m1_b8(mask_ints_v, 1, VLMAX);
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8m1(n);
            if (copied_per_iter == 0) break;
            vuint8m1_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle8.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse8.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle8_v_u8m1_m(mask, data, in, copied_per_iter);
            vse8_v_u8m1_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_MASKED
#if ENABLE_MASKED
void vector_memcpy_masked_e16m2(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    uint16_t mask_ints[128] = {0};
    const size_t VLMAX = vsetvlmax_e16m2();
    for (size_t i = 0; i < VLMAX; i++) {
        mask_ints[i] = i & 1;
    }
    vuint16m2_t mask_ints_v;
    #if USE_ASM_FOR_UNIT
    asm volatile ("vle16.v %0, (%1)" : "=vr"(mask_ints_v) : ASM_PREG(&mask_ints[0]));
    #else
    mask_ints_v = vle16_v_u16m2(mask_ints, VLMAX);
    #endif // USE_ASM_FOR_UNIT
    vbool8_t mask = vmseq_vx_u16m2_b8(mask_ints_v, 1, VLMAX);
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16m2(n);
            if (copied_per_iter == 0) break;
            vuint16m2_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle16.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse16.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle16_v_u16m2_m(mask, data, in, copied_per_iter);
            vse16_v_u16m2_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_MASKED
#if ENABLE_MASKED
void vector_memcpy_masked_e32m4(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    uint32_t mask_ints[128] = {0};
    const size_t VLMAX = vsetvlmax_e32m4();
    for (size_t i = 0; i < VLMAX; i++) {
        mask_ints[i] = i & 1;
    }
    vuint32m4_t mask_ints_v;
    #if USE_ASM_FOR_UNIT
    asm volatile ("vle32.v %0, (%1)" : "=vr"(mask_ints_v) : ASM_PREG(&mask_ints[0]));
    #else
    mask_ints_v = vle32_v_u32m4(mask_ints, VLMAX);
    #endif // USE_ASM_FOR_UNIT
    vbool8_t mask = vmseq_vx_u32m4_b8(mask_ints_v, 1, VLMAX);
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32m4(n);
            if (copied_per_iter == 0) break;
            vuint32m4_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle32.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse32.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle32_v_u32m4_m(mask, data, in, copied_per_iter);
            vse32_v_u32m4_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_MASKED
#if ENABLE_MASKED
void vector_memcpy_masked_e64m8(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    uint64_t mask_ints[128] = {0};
    const size_t VLMAX = vsetvlmax_e64m8();
    for (size_t i = 0; i < VLMAX; i++) {
        mask_ints[i] = i & 1;
    }
    vuint64m8_t mask_ints_v;
    #if USE_ASM_FOR_UNIT
    asm volatile ("vle64.v %0, (%1)" : "=vr"(mask_ints_v) : ASM_PREG(&mask_ints[0]));
    #else
    mask_ints_v = vle64_v_u64m8(mask_ints, VLMAX);
    #endif // USE_ASM_FOR_UNIT
    vbool8_t mask = vmseq_vx_u64m8_b8(mask_ints_v, 1, VLMAX);
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m8(n);
            if (copied_per_iter == 0) break;
            vuint64m8_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle64.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse64.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle64_v_u64m8_m(mask, data, in, copied_per_iter);
            vse64_v_u64m8_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_MASKED
#if ENABLE_MASKED && ENABLE_FRAC_LMUL
void vector_memcpy_masked_e32mf2(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    uint32_t mask_ints[128] = {0};
    const size_t VLMAX = vsetvlmax_e32mf2();
    for (size_t i = 0; i < VLMAX; i++) {
        mask_ints[i] = i & 1;
    }
    vuint32mf2_t mask_ints_v;
    #if USE_ASM_FOR_UNIT
    asm volatile ("vle32.v %0, (%1)" : "=vr"(mask_ints_v) : ASM_PREG(&mask_ints[0]));
    #else
    mask_ints_v = vle32_v_u32mf2(mask_ints, VLMAX);
    #endif // USE_ASM_FOR_UNIT
    vbool64_t mask = vmseq_vx_u32mf2_b64(mask_ints_v, 1, VLMAX);
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32mf2(n);
            if (copied_per_iter == 0) break;
            vuint32mf2_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle32.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse32.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle32_v_u32mf2_m(mask, data, in, copied_per_iter);
            vse32_v_u32mf2_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_MASKED && ENABLE_FRAC_LMUL
#if ENABLE_MASKED && ENABLE_FRAC_LMUL
void vector_memcpy_masked_e16mf4(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    uint16_t mask_ints[128] = {0};
    const size_t VLMAX = vsetvlmax_e16mf4();
    for (size_t i = 0; i < VLMAX; i++) {
        mask_ints[i] = i & 1;
    }
    vuint16mf4_t mask_ints_v;
    #if USE_ASM_FOR_UNIT
    asm volatile ("vle16.v %0, (%1)" : "=vr"(mask_ints_v) : ASM_PREG(&mask_ints[0]));
    #else
    mask_ints_v = vle16_v_u16mf4(mask_ints, VLMAX);
    #endif // USE_ASM_FOR_UNIT
    vbool64_t mask = vmseq_vx_u16mf4_b64(mask_ints_v, 1, VLMAX);
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16mf4(n);
            if (copied_per_iter == 0) break;
            vuint16mf4_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle16.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse16.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle16_v_u16mf4_m(mask, data, in, copied_per_iter);
            vse16_v_u16mf4_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_MASKED && ENABLE_FRAC_LMUL
#if ENABLE_MASKED && ENABLE_FRAC_LMUL
void vector_memcpy_masked_e8mf8(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    uint8_t mask_ints[128] = {0};
    const size_t VLMAX = vsetvlmax_e8mf8();
    for (size_t i = 0; i < VLMAX; i++) {
        mask_ints[i] = i & 1;
    }
    vuint8mf8_t mask_ints_v;
    #if USE_ASM_FOR_UNIT
    asm volatile ("vle8.v %0, (%1)" : "=vr"(mask_ints_v) : ASM_PREG(&mask_ints[0]));
    #else
    mask_ints_v = vle8_v_u8mf8(mask_ints, VLMAX);
    #endif // USE_ASM_FOR_UNIT
    vbool64_t mask = vmseq_vx_u8mf8_b64(mask_ints_v, 1, VLMAX);
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8mf8(n);
            if (copied_per_iter == 0) break;
            vuint8mf8_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle8.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse8.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle8_v_u8mf8_m(mask, data, in, copied_per_iter);
            vse8_v_u8mf8_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_MASKED && ENABLE_FRAC_LMUL
#if ENABLE_BYTEMASK
void vector_memcpy_masked_bytemask_load_e8m1(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    uint64_t mask_int = 0;
    const size_t VLMAX = vsetvlmax_e8m1();
    if (VLMAX > 64) return;
    for (size_t i = 0; i < VLMAX; i++) {
        mask_int |= (i & 1) << i;
    }
    vbool8_t mask;
    #if USE_ASM_FOR_BYTEMASK
    asm volatile ("vlm.v %0, (%1)" : "=vr"(mask) : ASM_PREG(&mask_int));
    #else
    mask = vlm_v_b8(&mask_int, VLMAX);
    #endif // USE_ASM_FOR_BYTEMASK
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8m1(n);
            if (copied_per_iter == 0) break;
            vuint8m1_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle8.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse8.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle8_v_u8m1_m(mask, data, in, copied_per_iter);
            vse8_v_u8m1_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_BYTEMASK
#if ENABLE_BYTEMASK
void vector_memcpy_masked_bytemask_load_e16m2(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    uint64_t mask_int = 0;
    const size_t VLMAX = vsetvlmax_e16m2();
    if (VLMAX > 64) return;
    for (size_t i = 0; i < VLMAX; i++) {
        mask_int |= (i & 1) << i;
    }
    vbool8_t mask;
    #if USE_ASM_FOR_BYTEMASK
    asm volatile ("vlm.v %0, (%1)" : "=vr"(mask) : ASM_PREG(&mask_int));
    #else
    mask = vlm_v_b8(&mask_int, VLMAX);
    #endif // USE_ASM_FOR_BYTEMASK
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16m2(n);
            if (copied_per_iter == 0) break;
            vuint16m2_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle16.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse16.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle16_v_u16m2_m(mask, data, in, copied_per_iter);
            vse16_v_u16m2_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_BYTEMASK
#if ENABLE_BYTEMASK
void vector_memcpy_masked_bytemask_load_e32m4(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    uint64_t mask_int = 0;
    const size_t VLMAX = vsetvlmax_e32m4();
    if (VLMAX > 64) return;
    for (size_t i = 0; i < VLMAX; i++) {
        mask_int |= (i & 1) << i;
    }
    vbool8_t mask;
    #if USE_ASM_FOR_BYTEMASK
    asm volatile ("vlm.v %0, (%1)" : "=vr"(mask) : ASM_PREG(&mask_int));
    #else
    mask = vlm_v_b8(&mask_int, VLMAX);
    #endif // USE_ASM_FOR_BYTEMASK
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32m4(n);
            if (copied_per_iter == 0) break;
            vuint32m4_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle32.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse32.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle32_v_u32m4_m(mask, data, in, copied_per_iter);
            vse32_v_u32m4_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_BYTEMASK
#if ENABLE_BYTEMASK
void vector_memcpy_masked_bytemask_load_e64m8(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    uint64_t mask_int = 0;
    const size_t VLMAX = vsetvlmax_e64m8();
    if (VLMAX > 64) return;
    for (size_t i = 0; i < VLMAX; i++) {
        mask_int |= (i & 1) << i;
    }
    vbool8_t mask;
    #if USE_ASM_FOR_BYTEMASK
    asm volatile ("vlm.v %0, (%1)" : "=vr"(mask) : ASM_PREG(&mask_int));
    #else
    mask = vlm_v_b8(&mask_int, VLMAX);
    #endif // USE_ASM_FOR_BYTEMASK
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m8(n);
            if (copied_per_iter == 0) break;
            vuint64m8_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle64.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse64.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle64_v_u64m8_m(mask, data, in, copied_per_iter);
            vse64_v_u64m8_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_BYTEMASK
#if ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
void vector_memcpy_masked_bytemask_load_e32mf2(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    uint64_t mask_int = 0;
    const size_t VLMAX = vsetvlmax_e32mf2();
    if (VLMAX > 64) return;
    for (size_t i = 0; i < VLMAX; i++) {
        mask_int |= (i & 1) << i;
    }
    vbool64_t mask;
    #if USE_ASM_FOR_BYTEMASK
    asm volatile ("vlm.v %0, (%1)" : "=vr"(mask) : ASM_PREG(&mask_int));
    #else
    mask = vlm_v_b64(&mask_int, VLMAX);
    #endif // USE_ASM_FOR_BYTEMASK
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32mf2(n);
            if (copied_per_iter == 0) break;
            vuint32mf2_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle32.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse32.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle32_v_u32mf2_m(mask, data, in, copied_per_iter);
            vse32_v_u32mf2_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
#if ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
void vector_memcpy_masked_bytemask_load_e16mf4(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    uint64_t mask_int = 0;
    const size_t VLMAX = vsetvlmax_e16mf4();
    if (VLMAX > 64) return;
    for (size_t i = 0; i < VLMAX; i++) {
        mask_int |= (i & 1) << i;
    }
    vbool64_t mask;
    #if USE_ASM_FOR_BYTEMASK
    asm volatile ("vlm.v %0, (%1)" : "=vr"(mask) : ASM_PREG(&mask_int));
    #else
    mask = vlm_v_b64(&mask_int, VLMAX);
    #endif // USE_ASM_FOR_BYTEMASK
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16mf4(n);
            if (copied_per_iter == 0) break;
            vuint16mf4_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle16.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse16.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle16_v_u16mf4_m(mask, data, in, copied_per_iter);
            vse16_v_u16mf4_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
#if ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
void vector_memcpy_masked_bytemask_load_e8mf8(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    uint64_t mask_int = 0;
    const size_t VLMAX = vsetvlmax_e8mf8();
    if (VLMAX > 64) return;
    for (size_t i = 0; i < VLMAX; i++) {
        mask_int |= (i & 1) << i;
    }
    vbool64_t mask;
    #if USE_ASM_FOR_BYTEMASK
    asm volatile ("vlm.v %0, (%1)" : "=vr"(mask) : ASM_PREG(&mask_int));
    #else
    mask = vlm_v_b64(&mask_int, VLMAX);
    #endif // USE_ASM_FOR_BYTEMASK
    #if USE_ASM_FOR_MASKED
    size_t mask_vlen = vsetvlmax_e8m1();
    asm volatile ("vmv.v.v v0, %0" :: "vr"(mask));
    #endif // USE_ASM_FOR_MASKED
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8mf8(n);
            if (copied_per_iter == 0) break;
            vuint8mf8_t data;
            #if USE_ASM_FOR_MASKED
            asm volatile ("vle8.v %0, (%1), v0.t" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("vse8.v %0, (%1), v0.t" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle8_v_u8mf8_m(mask, data, in, copied_per_iter);
            vse8_v_u8mf8_m(mask, out, data, copied_per_iter);
            #endif // USE_ASM_FOR_MASKED
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
#if ENABLE_SEGMENTED
void vector_memcpy_segmented_e8m2(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out[4]) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8m2(n);
            if (copied_per_iter == 0) break;
            #if USE_ASM_FOR_SEGMENTED
            asm volatile ("vlseg4e8.v v4, (%0)" :: ASM_PREG(in));
            asm volatile ("vse8.v v4, (%0)" :: ASM_PREG(out[0]));
            asm volatile ("vse8.v v6, (%0)" :: ASM_PREG(out[1]));
            asm volatile ("vse8.v v8, (%0)" :: ASM_PREG(out[2]));
            asm volatile ("vse8.v v10, (%0)" :: ASM_PREG(out[3]));
            #else
            vuint8m2_t r, g, b, a;
            vlseg4e8_v_u8m2(&r, &g, &b, &a, in, copied_per_iter);
            vse8_v_u8m2(out[0], r, copied_per_iter);
            vse8_v_u8m2(out[1], g, copied_per_iter);
            vse8_v_u8m2(out[2], b, copied_per_iter);
            vse8_v_u8m2(out[3], a, copied_per_iter);
            #endif // USE_ASM_FOR_SEGMENTED
            in += copied_per_iter * 4;
            for (int i = 0; i < 4; i++) {
                out[i] += copied_per_iter;
            }
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_SEGMENTED
#if ENABLE_SEGMENTED
void vector_memcpy_segmented_e16m2(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out[4]) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16m2(n);
            if (copied_per_iter == 0) break;
            #if USE_ASM_FOR_SEGMENTED
            asm volatile ("vlseg4e16.v v4, (%0)" :: ASM_PREG(in));
            asm volatile ("vse16.v v4, (%0)" :: ASM_PREG(out[0]));
            asm volatile ("vse16.v v6, (%0)" :: ASM_PREG(out[1]));
            asm volatile ("vse16.v v8, (%0)" :: ASM_PREG(out[2]));
            asm volatile ("vse16.v v10, (%0)" :: ASM_PREG(out[3]));
            #else
            vuint16m2_t r, g, b, a;
            vlseg4e16_v_u16m2(&r, &g, &b, &a, in, copied_per_iter);
            vse16_v_u16m2(out[0], r, copied_per_iter);
            vse16_v_u16m2(out[1], g, copied_per_iter);
            vse16_v_u16m2(out[2], b, copied_per_iter);
            vse16_v_u16m2(out[3], a, copied_per_iter);
            #endif // USE_ASM_FOR_SEGMENTED
            in += copied_per_iter * 4;
            for (int i = 0; i < 4; i++) {
                out[i] += copied_per_iter;
            }
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_SEGMENTED
#if ENABLE_SEGMENTED
void vector_memcpy_segmented_e32m2(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out[4]) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32m2(n);
            if (copied_per_iter == 0) break;
            #if USE_ASM_FOR_SEGMENTED
            asm volatile ("vlseg4e32.v v4, (%0)" :: ASM_PREG(in));
            asm volatile ("vse32.v v4, (%0)" :: ASM_PREG(out[0]));
            asm volatile ("vse32.v v6, (%0)" :: ASM_PREG(out[1]));
            asm volatile ("vse32.v v8, (%0)" :: ASM_PREG(out[2]));
            asm volatile ("vse32.v v10, (%0)" :: ASM_PREG(out[3]));
            #else
            vuint32m2_t r, g, b, a;
            vlseg4e32_v_u32m2(&r, &g, &b, &a, in, copied_per_iter);
            vse32_v_u32m2(out[0], r, copied_per_iter);
            vse32_v_u32m2(out[1], g, copied_per_iter);
            vse32_v_u32m2(out[2], b, copied_per_iter);
            vse32_v_u32m2(out[3], a, copied_per_iter);
            #endif // USE_ASM_FOR_SEGMENTED
            in += copied_per_iter * 4;
            for (int i = 0; i < 4; i++) {
                out[i] += copied_per_iter;
            }
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_SEGMENTED
#if ENABLE_SEGMENTED
void vector_memcpy_segmented_e64m2(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out[4]) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m2(n);
            if (copied_per_iter == 0) break;
            #if USE_ASM_FOR_SEGMENTED
            asm volatile ("vlseg4e64.v v4, (%0)" :: ASM_PREG(in));
            asm volatile ("vse64.v v4, (%0)" :: ASM_PREG(out[0]));
            asm volatile ("vse64.v v6, (%0)" :: ASM_PREG(out[1]));
            asm volatile ("vse64.v v8, (%0)" :: ASM_PREG(out[2]));
            asm volatile ("vse64.v v10, (%0)" :: ASM_PREG(out[3]));
            #else
            vuint64m2_t r, g, b, a;
            vlseg4e64_v_u64m2(&r, &g, &b, &a, in, copied_per_iter);
            vse64_v_u64m2(out[0], r, copied_per_iter);
            vse64_v_u64m2(out[1], g, copied_per_iter);
            vse64_v_u64m2(out[2], b, copied_per_iter);
            vse64_v_u64m2(out[3], a, copied_per_iter);
            #endif // USE_ASM_FOR_SEGMENTED
            in += copied_per_iter * 4;
            for (int i = 0; i < 4; i++) {
                out[i] += copied_per_iter;
            }
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_SEGMENTED
#if ENABLE_SEGMENTED && ENABLE_FRAC_LMUL
void vector_memcpy_segmented_e32mf2(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out[4]) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32mf2(n);
            if (copied_per_iter == 0) break;
            #if USE_ASM_FOR_SEGMENTED
            asm volatile ("vlseg4e32.v v4, (%0)" :: ASM_PREG(in));
            asm volatile ("vse32.v v4, (%0)" :: ASM_PREG(out[0]));
            asm volatile ("vse32.v v5, (%0)" :: ASM_PREG(out[1]));
            asm volatile ("vse32.v v6, (%0)" :: ASM_PREG(out[2]));
            asm volatile ("vse32.v v7, (%0)" :: ASM_PREG(out[3]));
            #else
            vuint32mf2_t r, g, b, a;
            vlseg4e32_v_u32mf2(&r, &g, &b, &a, in, copied_per_iter);
            vse32_v_u32mf2(out[0], r, copied_per_iter);
            vse32_v_u32mf2(out[1], g, copied_per_iter);
            vse32_v_u32mf2(out[2], b, copied_per_iter);
            vse32_v_u32mf2(out[3], a, copied_per_iter);
            #endif // USE_ASM_FOR_SEGMENTED
            in += copied_per_iter * 4;
            for (int i = 0; i < 4; i++) {
                out[i] += copied_per_iter;
            }
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_SEGMENTED && ENABLE_FRAC_LMUL
#if ENABLE_ASM_WHOLEREG
void vector_memcpy_wholereg_e64m1(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    const size_t VLMAX = vsetvlmax_e64m1();
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m1(n);
            if (copied_per_iter == 0) break;
            vuint64m1_t data;
            if (copied_per_iter == VLMAX) {
                asm volatile ("vl1r.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vs1r.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle64.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse64.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle64_v_u64m1(in, copied_per_iter);
                vse64_v_u64m1(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_ASM_WHOLEREG
#if ENABLE_ASM_WHOLEREG
void vector_memcpy_wholereg_e64m2(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    const size_t VLMAX = vsetvlmax_e64m2();
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m2(n);
            if (copied_per_iter == 0) break;
            vuint64m2_t data;
            if (copied_per_iter == VLMAX) {
                asm volatile ("vl2r.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vs2r.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle64.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse64.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle64_v_u64m2(in, copied_per_iter);
                vse64_v_u64m2(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_ASM_WHOLEREG
#if ENABLE_ASM_WHOLEREG
void vector_memcpy_wholereg_e64m4(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    const size_t VLMAX = vsetvlmax_e64m4();
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m4(n);
            if (copied_per_iter == 0) break;
            vuint64m4_t data;
            if (copied_per_iter == VLMAX) {
                asm volatile ("vl4r.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vs4r.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle64.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse64.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle64_v_u64m4(in, copied_per_iter);
                vse64_v_u64m4(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_ASM_WHOLEREG
#if ENABLE_ASM_WHOLEREG
void vector_memcpy_wholereg_e64m8(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    const size_t VLMAX = vsetvlmax_e64m8();
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m8(n);
            if (copied_per_iter == 0) break;
            vuint64m8_t data;
            if (copied_per_iter == VLMAX) {
                asm volatile ("vl8r.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vs8r.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            }
            else {
                #if USE_ASM_FOR_UNIT
                asm volatile ("vle64.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
                asm volatile ("vse64.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
                #else
                data = vle64_v_u64m8(in, copied_per_iter);
                vse64_v_u64m8(out, data, copied_per_iter);
                #endif // USE_ASM_FOR_UNIT
            }
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_ASM_WHOLEREG
#if ENABLE_FAULTONLYFIRST
void vector_memcpy_unit_stride_faultonlyfirst_e8m1(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8m1(n);
            if (copied_per_iter == 0) break;
            vuint8m1_t data;
            size_t new_vl;
            #if USE_ASM_FOR_FAULTONLYFIRST
            asm volatile ("vle8.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("csrr %0, vl" : "=r"(new_vl));
            if (new_vl != copied_per_iter) return;
            asm volatile ("vse8.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle8ff_v_u8m1(in, &new_vl, copied_per_iter);
            if (new_vl != copied_per_iter) return;
            vse8_v_u8m1(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_FAULTONLYFIRST
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_FAULTONLYFIRST
#if ENABLE_FAULTONLYFIRST
void vector_memcpy_unit_stride_faultonlyfirst_e16m2(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16m2(n);
            if (copied_per_iter == 0) break;
            vuint16m2_t data;
            size_t new_vl;
            #if USE_ASM_FOR_FAULTONLYFIRST
            asm volatile ("vle16.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("csrr %0, vl" : "=r"(new_vl));
            if (new_vl != copied_per_iter) return;
            asm volatile ("vse16.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle16ff_v_u16m2(in, &new_vl, copied_per_iter);
            if (new_vl != copied_per_iter) return;
            vse16_v_u16m2(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_FAULTONLYFIRST
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_FAULTONLYFIRST
#if ENABLE_FAULTONLYFIRST
void vector_memcpy_unit_stride_faultonlyfirst_e32m4(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32m4(n);
            if (copied_per_iter == 0) break;
            vuint32m4_t data;
            size_t new_vl;
            #if USE_ASM_FOR_FAULTONLYFIRST
            asm volatile ("vle32.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("csrr %0, vl" : "=r"(new_vl));
            if (new_vl != copied_per_iter) return;
            asm volatile ("vse32.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle32ff_v_u32m4(in, &new_vl, copied_per_iter);
            if (new_vl != copied_per_iter) return;
            vse32_v_u32m4(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_FAULTONLYFIRST
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_FAULTONLYFIRST
#if ENABLE_FAULTONLYFIRST
void vector_memcpy_unit_stride_faultonlyfirst_e64m8(size_t n, const uint64_t* __restrict__ in, uint64_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e64m8(n);
            if (copied_per_iter == 0) break;
            vuint64m8_t data;
            size_t new_vl;
            #if USE_ASM_FOR_FAULTONLYFIRST
            asm volatile ("vle64.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("csrr %0, vl" : "=r"(new_vl));
            if (new_vl != copied_per_iter) return;
            asm volatile ("vse64.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle64ff_v_u64m8(in, &new_vl, copied_per_iter);
            if (new_vl != copied_per_iter) return;
            vse64_v_u64m8(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_FAULTONLYFIRST
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_FAULTONLYFIRST
#if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
void vector_memcpy_unit_stride_faultonlyfirst_e32mf2(size_t n, const uint32_t* __restrict__ in, uint32_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e32mf2(n);
            if (copied_per_iter == 0) break;
            vuint32mf2_t data;
            size_t new_vl;
            #if USE_ASM_FOR_FAULTONLYFIRST
            asm volatile ("vle32.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("csrr %0, vl" : "=r"(new_vl));
            if (new_vl != copied_per_iter) return;
            asm volatile ("vse32.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle32ff_v_u32mf2(in, &new_vl, copied_per_iter);
            if (new_vl != copied_per_iter) return;
            vse32_v_u32mf2(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_FAULTONLYFIRST
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
#if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
void vector_memcpy_unit_stride_faultonlyfirst_e16mf4(size_t n, const uint16_t* __restrict__ in, uint16_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e16mf4(n);
            if (copied_per_iter == 0) break;
            vuint16mf4_t data;
            size_t new_vl;
            #if USE_ASM_FOR_FAULTONLYFIRST
            asm volatile ("vle16.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("csrr %0, vl" : "=r"(new_vl));
            if (new_vl != copied_per_iter) return;
            asm volatile ("vse16.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle16ff_v_u16mf4(in, &new_vl, copied_per_iter);
            if (new_vl != copied_per_iter) return;
            vse16_v_u16mf4(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_FAULTONLYFIRST
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
#if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
void vector_memcpy_unit_stride_faultonlyfirst_e8mf8(size_t n, const uint8_t* __restrict__ in, uint8_t* __restrict__ out) {
    while (1) {
         {
            size_t copied_per_iter = vsetvl_e8mf8(n);
            if (copied_per_iter == 0) break;
            vuint8mf8_t data;
            size_t new_vl;
            #if USE_ASM_FOR_FAULTONLYFIRST
            asm volatile ("vle8.v %0, (%1)" : "=vr"(data) : ASM_PREG(in));
            asm volatile ("csrr %0, vl" : "=r"(new_vl));
            if (new_vl != copied_per_iter) return;
            asm volatile ("vse8.v %0, (%1)" :: "vr"(data),  ASM_PREG(out));
            #else
            data = vle8ff_v_u8mf8(in, &new_vl, copied_per_iter);
            if (new_vl != copied_per_iter) return;
            vse8_v_u8mf8(out, data, copied_per_iter);
            #endif // USE_ASM_FOR_FAULTONLYFIRST
            in += copied_per_iter;
            out += copied_per_iter;
            n -= copied_per_iter;
        }
    }
}
#endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
#if ENABLE_FAULTONLYFIRST
int64_t vector_memcpy_boundary_faultonlyfirst_e8m1() {
    uint8_t* unmapped_ptr = (uint8_t*)&ramBoundary;
    const size_t VLMAX = vsetvlmax_e8m1();
    for (size_t i = 0; i < VLMAX; i++) {
        *(unmapped_ptr - VLMAX + i) = i;
    }
    for (size_t expected_num_copied = 1; expected_num_copied <= VLMAX; expected_num_copied++) {
        const uint8_t* in = unmapped_ptr - expected_num_copied;
        vsetvlmax_e8m1();
        size_t new_vl;
        #if USE_ASM_FOR_FAULTONLYFIRST
        asm volatile ("vle8ff.v v8, (%0)" :: ASM_PREG(in));
        asm volatile ("csrr %0, vl" : "=r"(new_vl));
        #else
        vle8ff_v_u8m1(in, &new_vl, VLMAX);
        #endif // USE_ASM_FOR_FAULTONLYFIRST
        if (new_vl != expected_num_copied) return 0;
    }
    return 1;
}
#endif // ENABLE_FAULTONLYFIRST
#if ENABLE_FAULTONLYFIRST
int64_t vector_memcpy_boundary_faultonlyfirst_e16m2() {
    uint16_t* unmapped_ptr = (uint16_t*)&ramBoundary;
    const size_t VLMAX = vsetvlmax_e16m2();
    for (size_t i = 0; i < VLMAX; i++) {
        *(unmapped_ptr - VLMAX + i) = i;
    }
    for (size_t expected_num_copied = 1; expected_num_copied <= VLMAX; expected_num_copied++) {
        const uint16_t* in = unmapped_ptr - expected_num_copied;
        vsetvlmax_e16m2();
        size_t new_vl;
        #if USE_ASM_FOR_FAULTONLYFIRST
        asm volatile ("vle16ff.v v8, (%0)" :: ASM_PREG(in));
        asm volatile ("csrr %0, vl" : "=r"(new_vl));
        #else
        vle16ff_v_u16m2(in, &new_vl, VLMAX);
        #endif // USE_ASM_FOR_FAULTONLYFIRST
        if (new_vl != expected_num_copied) return 0;
    }
    return 1;
}
#endif // ENABLE_FAULTONLYFIRST
#if ENABLE_FAULTONLYFIRST
int64_t vector_memcpy_boundary_faultonlyfirst_e32m4() {
    uint32_t* unmapped_ptr = (uint32_t*)&ramBoundary;
    const size_t VLMAX = vsetvlmax_e32m4();
    for (size_t i = 0; i < VLMAX; i++) {
        *(unmapped_ptr - VLMAX + i) = i;
    }
    for (size_t expected_num_copied = 1; expected_num_copied <= VLMAX; expected_num_copied++) {
        const uint32_t* in = unmapped_ptr - expected_num_copied;
        vsetvlmax_e32m4();
        size_t new_vl;
        #if USE_ASM_FOR_FAULTONLYFIRST
        asm volatile ("vle32ff.v v8, (%0)" :: ASM_PREG(in));
        asm volatile ("csrr %0, vl" : "=r"(new_vl));
        #else
        vle32ff_v_u32m4(in, &new_vl, VLMAX);
        #endif // USE_ASM_FOR_FAULTONLYFIRST
        if (new_vl != expected_num_copied) return 0;
    }
    return 1;
}
#endif // ENABLE_FAULTONLYFIRST
#if ENABLE_FAULTONLYFIRST
int64_t vector_memcpy_boundary_faultonlyfirst_e64m8() {
    uint64_t* unmapped_ptr = (uint64_t*)&ramBoundary;
    const size_t VLMAX = vsetvlmax_e64m8();
    for (size_t i = 0; i < VLMAX; i++) {
        *(unmapped_ptr - VLMAX + i) = i;
    }
    for (size_t expected_num_copied = 1; expected_num_copied <= VLMAX; expected_num_copied++) {
        const uint64_t* in = unmapped_ptr - expected_num_copied;
        vsetvlmax_e64m8();
        size_t new_vl;
        #if USE_ASM_FOR_FAULTONLYFIRST
        asm volatile ("vle64ff.v v8, (%0)" :: ASM_PREG(in));
        asm volatile ("csrr %0, vl" : "=r"(new_vl));
        #else
        vle64ff_v_u64m8(in, &new_vl, VLMAX);
        #endif // USE_ASM_FOR_FAULTONLYFIRST
        if (new_vl != expected_num_copied) return 0;
    }
    return 1;
}
#endif // ENABLE_FAULTONLYFIRST
#if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
int64_t vector_memcpy_boundary_faultonlyfirst_e32mf2() {
    uint32_t* unmapped_ptr = (uint32_t*)&ramBoundary;
    const size_t VLMAX = vsetvlmax_e32mf2();
    for (size_t i = 0; i < VLMAX; i++) {
        *(unmapped_ptr - VLMAX + i) = i;
    }
    for (size_t expected_num_copied = 1; expected_num_copied <= VLMAX; expected_num_copied++) {
        const uint32_t* in = unmapped_ptr - expected_num_copied;
        vsetvlmax_e32mf2();
        size_t new_vl;
        #if USE_ASM_FOR_FAULTONLYFIRST
        asm volatile ("vle32ff.v v8, (%0)" :: ASM_PREG(in));
        asm volatile ("csrr %0, vl" : "=r"(new_vl));
        #else
        vle32ff_v_u32mf2(in, &new_vl, VLMAX);
        #endif // USE_ASM_FOR_FAULTONLYFIRST
        if (new_vl != expected_num_copied) return 0;
    }
    return 1;
}
#endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
#if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
int64_t vector_memcpy_boundary_faultonlyfirst_e16mf4() {
    uint16_t* unmapped_ptr = (uint16_t*)&ramBoundary;
    const size_t VLMAX = vsetvlmax_e16mf4();
    for (size_t i = 0; i < VLMAX; i++) {
        *(unmapped_ptr - VLMAX + i) = i;
    }
    for (size_t expected_num_copied = 1; expected_num_copied <= VLMAX; expected_num_copied++) {
        const uint16_t* in = unmapped_ptr - expected_num_copied;
        vsetvlmax_e16mf4();
        size_t new_vl;
        #if USE_ASM_FOR_FAULTONLYFIRST
        asm volatile ("vle16ff.v v8, (%0)" :: ASM_PREG(in));
        asm volatile ("csrr %0, vl" : "=r"(new_vl));
        #else
        vle16ff_v_u16mf4(in, &new_vl, VLMAX);
        #endif // USE_ASM_FOR_FAULTONLYFIRST
        if (new_vl != expected_num_copied) return 0;
    }
    return 1;
}
#endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
#if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
int64_t vector_memcpy_boundary_faultonlyfirst_e8mf8() {
    uint8_t* unmapped_ptr = (uint8_t*)&ramBoundary;
    const size_t VLMAX = vsetvlmax_e8mf8();
    for (size_t i = 0; i < VLMAX; i++) {
        *(unmapped_ptr - VLMAX + i) = i;
    }
    for (size_t expected_num_copied = 1; expected_num_copied <= VLMAX; expected_num_copied++) {
        const uint8_t* in = unmapped_ptr - expected_num_copied;
        vsetvlmax_e8mf8();
        size_t new_vl;
        #if USE_ASM_FOR_FAULTONLYFIRST
        asm volatile ("vle8ff.v v8, (%0)" :: ASM_PREG(in));
        asm volatile ("csrr %0, vl" : "=r"(new_vl));
        #else
        vle8ff_v_u8mf8(in, &new_vl, VLMAX);
        #endif // USE_ASM_FOR_FAULTONLYFIRST
        if (new_vl != expected_num_copied) return 0;
    }
    return 1;
}
#endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL

#ifdef __cplusplus
extern "C" {;
#endif // __cplusplus
int main(void) {
    int64_t attempted = 0;
    int64_t successful = 0;
    #if ENABLE_UNIT
    attempted  |= 1ll << 0;
    successful |= vector_memcpy_harness_uint8_t(vector_memcpy_unit_stride_e8m1) << 0;
    #endif // ENABLE_UNIT
    
    #if ENABLE_UNIT
    attempted  |= 1ll << 1;
    successful |= vector_memcpy_harness_uint16_t(vector_memcpy_unit_stride_e16m2) << 1;
    #endif // ENABLE_UNIT
    
    #if ENABLE_UNIT
    attempted  |= 1ll << 2;
    successful |= vector_memcpy_harness_uint32_t(vector_memcpy_unit_stride_e32m4) << 2;
    #endif // ENABLE_UNIT
    
    #if ENABLE_UNIT
    attempted  |= 1ll << 3;
    successful |= vector_memcpy_harness_uint64_t(vector_memcpy_unit_stride_e64m8) << 3;
    #endif // ENABLE_UNIT
    
    #if ENABLE_UNIT && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 4;
    successful |= vector_memcpy_harness_uint32_t(vector_memcpy_unit_stride_e32mf2) << 4;
    #endif // ENABLE_UNIT && ENABLE_FRAC_LMUL
    
    #if ENABLE_UNIT && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 5;
    successful |= vector_memcpy_harness_uint16_t(vector_memcpy_unit_stride_e16mf4) << 5;
    #endif // ENABLE_UNIT && ENABLE_FRAC_LMUL
    
    #if ENABLE_UNIT && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 6;
    successful |= vector_memcpy_harness_uint8_t(vector_memcpy_unit_stride_e8mf8) << 6;
    #endif // ENABLE_UNIT && ENABLE_FRAC_LMUL
    
    #if ENABLE_STRIDED
    attempted  |= 1ll << 7;
    successful |= vector_memcpy_harness_uint8_t(vector_memcpy_strided_e8m1) << 7;
    #endif // ENABLE_STRIDED
    
    #if ENABLE_STRIDED
    attempted  |= 1ll << 8;
    successful |= vector_memcpy_harness_uint16_t(vector_memcpy_strided_e16m2) << 8;
    #endif // ENABLE_STRIDED
    
    #if ENABLE_STRIDED
    attempted  |= 1ll << 9;
    successful |= vector_memcpy_harness_uint32_t(vector_memcpy_strided_e32m4) << 9;
    #endif // ENABLE_STRIDED
    
    #if ENABLE_STRIDED
    attempted  |= 1ll << 10;
    successful |= vector_memcpy_harness_uint64_t(vector_memcpy_strided_e64m8) << 10;
    #endif // ENABLE_STRIDED
    
    #if ENABLE_STRIDED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 11;
    successful |= vector_memcpy_harness_uint32_t(vector_memcpy_strided_e32mf2) << 11;
    #endif // ENABLE_STRIDED && ENABLE_FRAC_LMUL
    
    #if ENABLE_STRIDED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 12;
    successful |= vector_memcpy_harness_uint16_t(vector_memcpy_strided_e16mf4) << 12;
    #endif // ENABLE_STRIDED && ENABLE_FRAC_LMUL
    
    #if ENABLE_STRIDED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 13;
    successful |= vector_memcpy_harness_uint8_t(vector_memcpy_strided_e8mf8) << 13;
    #endif // ENABLE_STRIDED && ENABLE_FRAC_LMUL
    
    #if ENABLE_INDEXED
    attempted  |= 1ll << 14;
    successful |= vector_memcpy_harness_uint8_t(vector_memcpy_indexed_e8m1) << 14;
    #endif // ENABLE_INDEXED
    
    #if ENABLE_INDEXED
    attempted  |= 1ll << 15;
    successful |= vector_memcpy_harness_uint16_t(vector_memcpy_indexed_e16m2) << 15;
    #endif // ENABLE_INDEXED
    
    #if ENABLE_INDEXED
    attempted  |= 1ll << 16;
    successful |= vector_memcpy_harness_uint32_t(vector_memcpy_indexed_e32m4) << 16;
    #endif // ENABLE_INDEXED
    
    #if ENABLE_INDEXED
    attempted  |= 1ll << 17;
    successful |= vector_memcpy_harness_uint64_t(vector_memcpy_indexed_e64m8) << 17;
    #endif // ENABLE_INDEXED
    
    #if ENABLE_INDEXED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 18;
    successful |= vector_memcpy_harness_uint32_t(vector_memcpy_indexed_e32mf2) << 18;
    #endif // ENABLE_INDEXED && ENABLE_FRAC_LMUL
    
    #if ENABLE_INDEXED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 19;
    successful |= vector_memcpy_harness_uint16_t(vector_memcpy_indexed_e16mf4) << 19;
    #endif // ENABLE_INDEXED && ENABLE_FRAC_LMUL
    
    #if ENABLE_INDEXED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 20;
    successful |= vector_memcpy_harness_uint8_t(vector_memcpy_indexed_e8mf8) << 20;
    #endif // ENABLE_INDEXED && ENABLE_FRAC_LMUL
    
    #if ENABLE_MASKED
    attempted  |= 1ll << 21;
    successful |= vector_memcpy_masked_harness_uint8_t(vector_memcpy_masked_e8m1) << 21;
    #endif // ENABLE_MASKED
    
    #if ENABLE_MASKED
    attempted  |= 1ll << 22;
    successful |= vector_memcpy_masked_harness_uint16_t(vector_memcpy_masked_e16m2) << 22;
    #endif // ENABLE_MASKED
    
    #if ENABLE_MASKED
    attempted  |= 1ll << 23;
    successful |= vector_memcpy_masked_harness_uint32_t(vector_memcpy_masked_e32m4) << 23;
    #endif // ENABLE_MASKED
    
    #if ENABLE_MASKED
    attempted  |= 1ll << 24;
    successful |= vector_memcpy_masked_harness_uint64_t(vector_memcpy_masked_e64m8) << 24;
    #endif // ENABLE_MASKED
    
    #if ENABLE_MASKED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 25;
    successful |= vector_memcpy_masked_harness_uint32_t(vector_memcpy_masked_e32mf2) << 25;
    #endif // ENABLE_MASKED && ENABLE_FRAC_LMUL
    
    #if ENABLE_MASKED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 26;
    successful |= vector_memcpy_masked_harness_uint16_t(vector_memcpy_masked_e16mf4) << 26;
    #endif // ENABLE_MASKED && ENABLE_FRAC_LMUL
    
    #if ENABLE_MASKED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 27;
    successful |= vector_memcpy_masked_harness_uint8_t(vector_memcpy_masked_e8mf8) << 27;
    #endif // ENABLE_MASKED && ENABLE_FRAC_LMUL
    
    #if ENABLE_BYTEMASK
    attempted  |= 1ll << 28;
    successful |= vector_memcpy_masked_harness_uint8_t(vector_memcpy_masked_bytemask_load_e8m1) << 28;
    #endif // ENABLE_BYTEMASK
    
    #if ENABLE_BYTEMASK
    attempted  |= 1ll << 29;
    successful |= vector_memcpy_masked_harness_uint16_t(vector_memcpy_masked_bytemask_load_e16m2) << 29;
    #endif // ENABLE_BYTEMASK
    
    #if ENABLE_BYTEMASK
    attempted  |= 1ll << 30;
    successful |= vector_memcpy_masked_harness_uint32_t(vector_memcpy_masked_bytemask_load_e32m4) << 30;
    #endif // ENABLE_BYTEMASK
    
    #if ENABLE_BYTEMASK
    attempted  |= 1ll << 31;
    successful |= vector_memcpy_masked_harness_uint64_t(vector_memcpy_masked_bytemask_load_e64m8) << 31;
    #endif // ENABLE_BYTEMASK
    
    #if ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 32;
    successful |= vector_memcpy_masked_harness_uint32_t(vector_memcpy_masked_bytemask_load_e32mf2) << 32;
    #endif // ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
    
    #if ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 33;
    successful |= vector_memcpy_masked_harness_uint16_t(vector_memcpy_masked_bytemask_load_e16mf4) << 33;
    #endif // ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
    
    #if ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 34;
    successful |= vector_memcpy_masked_harness_uint8_t(vector_memcpy_masked_bytemask_load_e8mf8) << 34;
    #endif // ENABLE_BYTEMASK && ENABLE_FRAC_LMUL
    
    #if ENABLE_SEGMENTED
    attempted  |= 1ll << 35;
    successful |= vector_memcpy_segmented_harness_uint8_t(vector_memcpy_segmented_e8m2) << 35;
    #endif // ENABLE_SEGMENTED
    
    #if ENABLE_SEGMENTED
    attempted  |= 1ll << 36;
    successful |= vector_memcpy_segmented_harness_uint16_t(vector_memcpy_segmented_e16m2) << 36;
    #endif // ENABLE_SEGMENTED
    
    #if ENABLE_SEGMENTED
    attempted  |= 1ll << 37;
    successful |= vector_memcpy_segmented_harness_uint32_t(vector_memcpy_segmented_e32m2) << 37;
    #endif // ENABLE_SEGMENTED
    
    #if ENABLE_SEGMENTED
    attempted  |= 1ll << 38;
    successful |= vector_memcpy_segmented_harness_uint64_t(vector_memcpy_segmented_e64m2) << 38;
    #endif // ENABLE_SEGMENTED
    
    #if ENABLE_SEGMENTED && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 39;
    successful |= vector_memcpy_segmented_harness_uint32_t(vector_memcpy_segmented_e32mf2) << 39;
    #endif // ENABLE_SEGMENTED && ENABLE_FRAC_LMUL
    
    #if ENABLE_ASM_WHOLEREG
    attempted  |= 1ll << 40;
    successful |= vector_memcpy_harness_uint64_t(vector_memcpy_wholereg_e64m1) << 40;
    #endif // ENABLE_ASM_WHOLEREG
    
    #if ENABLE_ASM_WHOLEREG
    attempted  |= 1ll << 41;
    successful |= vector_memcpy_harness_uint64_t(vector_memcpy_wholereg_e64m2) << 41;
    #endif // ENABLE_ASM_WHOLEREG
    
    #if ENABLE_ASM_WHOLEREG
    attempted  |= 1ll << 42;
    successful |= vector_memcpy_harness_uint64_t(vector_memcpy_wholereg_e64m4) << 42;
    #endif // ENABLE_ASM_WHOLEREG
    
    #if ENABLE_ASM_WHOLEREG
    attempted  |= 1ll << 43;
    successful |= vector_memcpy_harness_uint64_t(vector_memcpy_wholereg_e64m8) << 43;
    #endif // ENABLE_ASM_WHOLEREG
    
    #if ENABLE_FAULTONLYFIRST
    attempted  |= 1ll << 44;
    successful |= vector_memcpy_harness_uint8_t(vector_memcpy_unit_stride_faultonlyfirst_e8m1) << 44;
    #endif // ENABLE_FAULTONLYFIRST
    
    #if ENABLE_FAULTONLYFIRST
    attempted  |= 1ll << 45;
    successful |= vector_memcpy_harness_uint16_t(vector_memcpy_unit_stride_faultonlyfirst_e16m2) << 45;
    #endif // ENABLE_FAULTONLYFIRST
    
    #if ENABLE_FAULTONLYFIRST
    attempted  |= 1ll << 46;
    successful |= vector_memcpy_harness_uint32_t(vector_memcpy_unit_stride_faultonlyfirst_e32m4) << 46;
    #endif // ENABLE_FAULTONLYFIRST
    
    #if ENABLE_FAULTONLYFIRST
    attempted  |= 1ll << 47;
    successful |= vector_memcpy_harness_uint64_t(vector_memcpy_unit_stride_faultonlyfirst_e64m8) << 47;
    #endif // ENABLE_FAULTONLYFIRST
    
    #if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 48;
    successful |= vector_memcpy_harness_uint32_t(vector_memcpy_unit_stride_faultonlyfirst_e32mf2) << 48;
    #endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    
    #if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 49;
    successful |= vector_memcpy_harness_uint16_t(vector_memcpy_unit_stride_faultonlyfirst_e16mf4) << 49;
    #endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    
    #if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 50;
    successful |= vector_memcpy_harness_uint8_t(vector_memcpy_unit_stride_faultonlyfirst_e8mf8) << 50;
    #endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    
    #if ENABLE_FAULTONLYFIRST
    attempted  |= 1ll << 51;
    successful |= vector_memcpy_boundary_faultonlyfirst_e8m1() << 51;
    #endif // ENABLE_FAULTONLYFIRST
    
    #if ENABLE_FAULTONLYFIRST
    attempted  |= 1ll << 52;
    successful |= vector_memcpy_boundary_faultonlyfirst_e16m2() << 52;
    #endif // ENABLE_FAULTONLYFIRST
    
    #if ENABLE_FAULTONLYFIRST
    attempted  |= 1ll << 53;
    successful |= vector_memcpy_boundary_faultonlyfirst_e32m4() << 53;
    #endif // ENABLE_FAULTONLYFIRST
    
    #if ENABLE_FAULTONLYFIRST
    attempted  |= 1ll << 54;
    successful |= vector_memcpy_boundary_faultonlyfirst_e64m8() << 54;
    #endif // ENABLE_FAULTONLYFIRST
    
    #if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 55;
    successful |= vector_memcpy_boundary_faultonlyfirst_e32mf2() << 55;
    #endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    
    #if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 56;
    successful |= vector_memcpy_boundary_faultonlyfirst_e16mf4() << 56;
    #endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    
    #if ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    attempted  |= 1ll << 57;
    successful |= vector_memcpy_boundary_faultonlyfirst_e8mf8() << 57;
    #endif // ENABLE_FAULTONLYFIRST && ENABLE_FRAC_LMUL
    
    *(&outputAttempted) = attempted;
    *(&outputSucceeded) = successful;
    return 0;
}
#ifdef __cplusplus
};
#endif // __cplusplus
